{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpTODKmXT6My",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8ff9acbf-092c-4cc1-e55d-85862a4335f8"
      },
      "source": [
        "import lightgbm as lgb\n",
        "import logging\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive  \n",
        "from lightgbm.sklearn import LGBMClassifier\n",
        "from scipy import stats\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6N3alkMUIBx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc47f1b7-33b9-44e9-a8b9-21f822fd0e90"
      },
      "source": [
        "drive.mount(\"/content/drive\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY7W8soTUJsM"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/ks-projects-201801.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLkQKlhvU4Yd"
      },
      "source": [
        "print(data.info())\n",
        "data[\"category\"].value_counts().plot.barh(figsize=(25,35))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpFVvc95U6QU"
      },
      "source": [
        "# Let's get an idea as to the distribution of different outcomes\n",
        "data[\"state\"].value_counts().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbge6z93lZZq"
      },
      "source": [
        "data.head()\n",
        "categorical_features = [\"category\", \"main_category\", \"country\"]\n",
        "\n",
        "numeric_features = [\"usd_pledged_real\", \"usd_goal_real\", \"backers\"]\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "                (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
        "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "                (\"imputer\", SimpleImputer(fill_value=\"median\")),\n",
        "                (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "          (\"num\", numeric_transformer, numeric_features),\n",
        "          (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply relevant pipeline operations to each of the column types\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfnRr8ycU8R1"
      },
      "source": [
        "Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8gsCVMvU-JX"
      },
      "source": [
        "# First, let's save the categorical features\n",
        "data.head()\n",
        "categorical_features = [\"category\", \"main_category\", \"country\"]\n",
        "\n",
        "numeric_features = [\"usd_pledged_real\", \"usd_goal_real\", \"backers\"]\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "                (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
        "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "                (\"imputer\", SimpleImputer(fill_value=\"median\")),\n",
        "                (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "          (\"num\", numeric_transformer, numeric_features),\n",
        "          (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply relevant pipeline operations to each of the column types\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5y_I8UwVDJl"
      },
      "source": [
        "# Let's make the target attribute a bit prettier, and split the data into train/test\n",
        "# Drop live projects\n",
        "data = data.query('state != \"live\" and state != \"undefined\"')\n",
        "\n",
        "# Add outcome column, \"successful\" == 1, others are 0\n",
        "data = data.assign(outcome=(data['state'] == 'successful').astype(int))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmvxIj8kVFgq"
      },
      "source": [
        "# data = data.drop([\"state\", \"ID\", \"name\", \"currency\", \"deadline\", \"goal\", \"launched\",\"pledged\", \"usd pledged\"], axis=1)\n",
        "\n",
        "y = data.outcome\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3)\n",
        "\n",
        "# Select features from sets to be used\n",
        "X_train = X_train[[*numeric_features, *categorical_features]]\n",
        "X_test = X_test[[*numeric_features, *categorical_features]]\n",
        "\n",
        "# X_train.drop([\"outcome\"], axis=1)\n",
        "# for each in [X_train, X_test]:\n",
        "#     print(f\"Outcome fraction = {each.outcome.mean():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEfKvk92oGQd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "0923d9e3-8f5a-4d7c-f79a-9bfe775135ea"
      },
      "source": [
        "# Apply the transformers to the columns of the dataset\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "          (\"num\", numeric_transformer, numeric_features),\n",
        "          (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply relevant pipeline operations to each of the column types\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "X_train = data.iloc[\"usd_pledged_real\"]\n",
        "lir = LinearRegression()\n",
        "lir.fit()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b91b1ad28ff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m preprocessor = ColumnTransformer(\n\u001b[1;32m      2\u001b[0m     transformers=[\n\u001b[0;32m----> 3\u001b[0;31m           \u001b[0;34m(\u001b[0m\u001b[0;34m\"num\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m           \u001b[0;34m(\u001b[0m\u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'numeric_transformer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M35asDTYVHe6"
      },
      "source": [
        "# Apply the transformers to the columns of the dataset\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "          (\"num\", numeric_transformer, numeric_features),\n",
        "          (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply relevant pipeline operations to each of the column types\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Build classification pipeline for LightGBM\n",
        "regr = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('regressor', LGBMClassifier(boosting_type='gbdt',\n",
        "                        objective='binary'\n",
        "                      ))])\n",
        "\n",
        "\n",
        "# Select features from sets to be used\n",
        "# X_test.drop([\"outcome\"], axis=1)\n",
        "\n",
        "# `__` denotes attribute \n",
        "# (e.g. regressor__n_estimators means the `n_estimators` param for `regressor`\n",
        "#  which is our xgb).\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['median'],\n",
        "    'regressor__num_leaves': [30, 45],\n",
        "    'regressor__n_estimators': [200, 300],\n",
        "    'regressor__max_depth': [10, 15],\n",
        "    'regressor__learning_rate': [0.03, 0.05],\n",
        "    'regressor__min_data_in_leaf' : [10, 20],\n",
        "}\n",
        "\n",
        "# Grid search the parameters above, 3-fold cv\n",
        "grid_search = GridSearchCV(\n",
        "    regr, param_grid, cv=3, verbose=3, n_jobs=1, \n",
        "    scoring='accuracy')\n",
        "\n",
        "# Parameter optimization and model fitting\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp496OQaVKvg"
      },
      "source": [
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TstHQ-65VMrA"
      },
      "source": [
        "y_pred = pd.Series(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWLaMlCWVOZU"
      },
      "source": [
        "# this line crashes it because it runs out of ram... not sure what to do about that\n",
        "#finished = X_test.append(y_pred, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbO_zpTAVR36"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}